{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirments/ Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "service_account_key_path = \"gs://auth_service_key_seriousprojectid/serious-unison-441416-j6-556d0d88d23e.json\"\n",
    "\n",
    "# Initialize Spark session with GCS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC fetch Updatedrecords\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", service_account_key_path) \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"2000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "gcs_bucket = \"gs://nyc-311-requests/raw\"\n",
    "processed_gcs_bucket= \"gs://nyc-311-requests/transformed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allocate the parquetfile for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the existing Parquet file\n",
    "df=spark.read.parquet('gs://nyc-311-requests/raw/nyc311-datafetch-2025.parquet')\n",
    "df.show(1)\n",
    "print('suceessfully loaded the parquet file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_df=df\n",
    "\n",
    "#remove the null values from Unique_key (PK)\n",
    "tran_df=df.filter(df['unique_key'].isNotNull())\n",
    "\n",
    "\n",
    "# change of datatype (timestamp) by datecolumns\n",
    "date_columns = [\n",
    "    'created_date',\n",
    "    'closed_date',\n",
    "    'resolution_action_updated_date',\n",
    "    'due_date'\n",
    "]\n",
    "\n",
    "for column in date_columns:\n",
    "    tran_df = tran_df.withColumn(column, to_timestamp(col(column), \"yyyy-MM-dd'T'HH:mm:ss.SSS\"))\n",
    "##check for the nulls in created_date if any then recoords had the wrong datatype convertion\n",
    "\n",
    "\n",
    "# change of datatype (doubletype) by geolocation\n",
    "geo_columns = [\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "for column in geo_columns:\n",
    "  tran_df = tran_df.withColumn(column, col(column).cast(\"double\"))\n",
    "\n",
    "tran_df=tran_df.withColumn('location_type', lower(col('location_type')))\n",
    "\n",
    "\n",
    "# null to specified string\n",
    "tran_df = tran_df.fillna({'borough': 'Unspecified'})\n",
    "\n",
    "# replace null with N/A\n",
    "null_string_columns= ['descriptor', 'resolution_description',\n",
    "    'location_type',\n",
    "    'facility_type',\n",
    "    'vehicle_type',\n",
    "    'taxi_company_borough',\n",
    "    'taxi_pick_up_location',\n",
    "    'bridge_highway_direction',\n",
    "    'bridge_highway_name',\n",
    "    'bridge_highway_segment',\n",
    "    'road_ramp',\n",
    "    'address_type',\n",
    "    'street_name',\n",
    "    'city',\n",
    "    'incident_address',\n",
    "    'incident_zip',\n",
    "    'bbl'\n",
    "]\n",
    "\n",
    "\n",
    "for column in null_string_columns:\n",
    "    tran_df = tran_df.fillna({column: \"N/A\" for column in null_string_columns})\n",
    "\n",
    "\n",
    "# checks the of cleaned data\n",
    "tran_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize the schemas (date)\n",
    "\n",
    "Identification_columns = [\n",
    "    'unique_key',\n",
    "    'created_date',\n",
    "    'status',\n",
    "    'closed_date',\n",
    "    'resolution_action_updated_date',\n",
    "    'due_date',\n",
    "    'resolution_description',\n",
    "    'open_data_channel_type'\n",
    "]\n",
    "\n",
    "identification_df=tran_df.select(Identification_columns)\n",
    "identification_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize the schemas (location)\n",
    "\n",
    "location_columns = [\n",
    "    'unique_key',\n",
    "    'location_type', #Street/Sidewalk\n",
    "    'address_type',  # ADDRESS\n",
    "    'borough',       #QUEENS\n",
    "    'community_board', # 10 QUEENS\n",
    "    'street_name',      #  80 STREET\n",
    "    'city',             #HOWARD BEACH\n",
    "    'incident_address', # 153-39 80 STREET\n",
    "    'incident_zip',     #  11414\n",
    "    'bbl',              # 4114420043\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "location_df=tran_df.select(location_columns)\n",
    "location_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the schemas (compliant)\n",
    "\n",
    "complaint_columns = [\n",
    "    'unique_key',\n",
    "    'agency',\n",
    "    'agency_name',\n",
    "    'complaint_type',\n",
    "    'descriptor',\n",
    "    'location_type',\n",
    "    'facility_type',\n",
    "    'vehicle_type',\n",
    "    'taxi_company_borough',\n",
    "    'taxi_pick_up_location',\n",
    "    'bridge_highway_direction',\n",
    "    'bridge_highway_name',\n",
    "    'bridge_highway_segment',\n",
    "    'road_ramp',\n",
    "    'park_facility_name',\n",
    "    'park_borough'\n",
    "]\n",
    "\n",
    "complaint_df=tran_df.select(complaint_columns)\n",
    "complaint_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to do basic cleaning as main dataframe \n",
    "identification_df.dropDuplicates()\n",
    "complaint_df.dropDuplicates()\n",
    "location_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality checks(validations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if records count as same across the transformed tables/\n",
    "print(identification_df.count())\n",
    "print(complaint_df.count())\n",
    "location_df.count()\n",
    "\n",
    "\n",
    "#check for nulls in each transformed tables\n",
    "identification_df.select([count(when(isnull(c), c)).alias(c) for c in identification_df.columns]).show()\n",
    "complaint_df.select([count(when(isnull(c), c)).alias(c) for c in complaint_df.columns]).show()\n",
    "location_df.select([count(when(isnull(c), c)).alias(c) for c in location_df.columns]).show()\n",
    "\n",
    "\n",
    "# check for primary key\n",
    "duplicate_count = tran_df.groupBy('unique_key').count().filter(col('count')>1)\n",
    "duplicate_count.show()\n",
    "\n",
    "\n",
    "#validates range of dates in the tables\n",
    "identification_df.select(min('created_date')).show()\n",
    "identification_df.select(max('created_date')).show()\n",
    "\n",
    "\n",
    "\n",
    "##check for the nulls in created_date if any then recoords had the wrong datatype convertion\n",
    "\n",
    "#checks the address_type\n",
    "location_df.select('address_type').distinct().show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "location_df.select('location_type').distinct().show()\n",
    "identification_df.select('status').distinct().show()\n",
    "complaint_df.select('vehicle_type').distinct().show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# validates locations in NYC area\n",
    "invalid_geo_count = location_df.filter(\n",
    "    (col('latitude') < 40.4774) | (col('latitude') > 40.9176) |\n",
    "    (col('longitude') < -74.2591) | (col('longitude') > -73.7002)\n",
    ").count()\n",
    "invalid_geo_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "location_df.filter(\n",
    "    (col('incident_zip') != \"N/A\") |\n",
    "    (col('incident_zip') < 10001) | (col('incident_zip') > 11699)\n",
    ").count()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_agencies = [\n",
    "    'DEP', 'DPR', 'DOB', 'DOS', 'DOT', 'HPD', 'TLC',\n",
    "    'EDC', 'NYPD', 'DOE', 'DOHMH', 'DSNY', 'DCWP', 'DHS', 'OTI'\n",
    "]  # Example list of valid agencies\n",
    "invalid_agencies = complaint_df.filter(~col('agency').isin(valid_agencies))\n",
    "invalid_agencies.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to cloud storage\n",
    "\n",
    "output_path = \"gs://nyc-311-requests/transformed/\"\n",
    "identification_df.coalesce(1).write.parquet(\"gs://nyc-311-requests/transformed/identification.parquet\", mode=\"append\")\n",
    "location_df.coalesce(1).write.parquet(\"gs://nyc-311-requests/transformed/location.parquet\", mode=\"append\")\n",
    "complaint_df.coalesce(1).write.parquet(\"gs://nyc-311-requests/transformed/complaint.parquet\", mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to BigQuery as tables\n",
    "\n",
    "# Define the BigQuery project ID, dataset, and table\n",
    "project_id = \"serious-unison-441416-j6\"\n",
    "dataset_name = \"nyc_311requests\"\n",
    "table_name = \"case_details\"  # Existing table name\n",
    "\n",
    "# Define the GCS path for temporary storage\n",
    "transformed_gcs_output_path = \"gs://nyc-311-requests/transformed/jan3\"\n",
    "\n",
    "# Write DataFrame to BigQuery with append mode\n",
    "identification_df.write.format(\"bigquery\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_name) \\\n",
    "    .option(\"table\", table_name) \\\n",
    "    .option(\"temporaryGcsBucket\", transformed_gcs_output_path) \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
    "    .option(\"writeDisposition\", \"WRITE_APPEND\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data appended to table ....'case_details'.... in BigQuery successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BigQuery project ID, dataset, and table\n",
    "project_id = \"serious-unison-441416-j6\"\n",
    "dataset_name = \"nyc_311requests\"\n",
    "table_name = \"location\"  # New table name\n",
    "\n",
    "location_df.write.format(\"bigquery\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_name) \\\n",
    "    .option(\"table\", table_name) \\\n",
    "    .option(\"temporaryGcsBucket\", transformed_gcs_output_path) \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
    "    .option(\"writeDisposition\", \"WRITE_APPEND\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data appended to table ....'location'.... in BigQuery successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BigQuery project ID, dataset, and table\n",
    "project_id = \"serious-unison-441416-j6\"\n",
    "dataset_name = \"nyc_311requests\"\n",
    "table_name = \"complaint\"  # New table name\n",
    "\n",
    "complaint_df.write.format(\"bigquery\") \\\n",
    "    .option(\"project\", project_id) \\\n",
    "    .option(\"dataset\", dataset_name) \\\n",
    "    .option(\"table\", table_name) \\\n",
    "    .option(\"temporaryGcsBucket\", transformed_gcs_output_path) \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
    "    .option(\"writeDisposition\", \"WRITE_APPEND\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data appended to table ....'complaint'.... in BigQuery successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
